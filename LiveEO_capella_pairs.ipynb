{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cc8a04a-fb5e-4c92-99f6-99a3a9339edb",
   "metadata": {},
   "source": [
    "# Title: LiveEO case study - \"finding Capella (SAR) image pairs\"\n",
    "#### Date: 16.09.2025\n",
    "#### Author: Emma Underwood\n",
    "##### Workflow type: Pre-processing\n",
    "##### Aim of workflow: Filtering image pairs to become training samples for a change detection model\n",
    "##### Inputs: Parquet files: 1 x SAR images set (Capella), 1 x SAR images metadata\n",
    "##### Outputs: Summary statistics, methods and findings report (html), list of Capella pairs (as parquet, and human-friendly csv), web map of their distribution (html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26837b4d-82cd-47ca-9696-a8c0fe9916b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## STEP 1: Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea550dfe-5a44-412c-8732-20f9ea0e4227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports loaded\n",
      "✓ DuckDB spatial extension loaded\n",
      "  ... Systems ready!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Step 1: Import all tools required for workflow\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core data handling - prioritize DuckDB for large data\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "from shapely.geometry import shape, mapping\n",
    "\n",
    "# Spatial operations\n",
    "from pyproj import CRS, Transformer\n",
    "\n",
    "# visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "# Date handling\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Utilities\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure pandas for display only\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.options.mode.copy_on_write = True\n",
    "\n",
    "# Single DuckDB connection for entire workflow\n",
    "conn = duckdb.connect()\n",
    "conn.execute(\"INSTALL spatial; LOAD spatial;\")\n",
    "\n",
    "print(\"✓ All imports loaded\")\n",
    "print(\"✓ DuckDB spatial extension loaded\")\n",
    "print(\"  ... Systems ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dc8f32c-18b0-42c4-a0e3-60f3e2417fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All required files found\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set up project directory structure and configuration parameters\n",
    "Create organised folder structure, define file paths, processing parameters\n",
    "\"\"\"\n",
    "\n",
    "# Local git location - all relative paths after here\n",
    "WORKING_DIR = Path(r\"C:\\Users\\Emma Underwood\\Documents\\GitHub\\LiveEO\")\n",
    "#WORKING_DIR = Path.cwd()  # Current directory\n",
    "\n",
    "def setup_project_structure(base_path):\n",
    "    \"\"\"Create project directory structure within git repo\"\"\"\n",
    "    directories = [\n",
    "        \"data/raw\",                 # Original parquet files - add to .gitignore\n",
    "        \"data/output\",              # Final outputs (pairs files)\n",
    "        \"data/output/derived\",      # Processed intermediate files\n",
    "        \"reports\",                  # HTML/PDF reports\n",
    "        \"logs\"                      # Processing logs\n",
    "    ]\n",
    "    \n",
    "    # Create all directories relative to base path\n",
    "    for dir_path in directories:\n",
    "        (base_path / dir_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create .gitignore in project root for the big files\n",
    "    gitignore_path = base_path / '.gitignore'\n",
    "    gitignore_content = \"\"\"\n",
    "# Large data files - don't commit!\n",
    "data/raw/*.parquet\n",
    "data/raw/*.tif  \n",
    "data/raw/*.zip\n",
    "\n",
    "# Temporary processing files\n",
    "*.tmp\n",
    "*.log\n",
    ".ipynb_checkpoints/\n",
    "\n",
    "# OS generated files\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\"\"\"\n",
    "    \n",
    "    if not gitignore_path.exists():\n",
    "        with open(gitignore_path, 'w') as f:\n",
    "            f.write(gitignore_content)\n",
    "    \n",
    "    print(f\"Project structure created in: {base_path}\")\n",
    "    return True\n",
    "\n",
    "# Configuration class - all paths relative to working directory\n",
    "class Config:\n",
    "    \"\"\"Configuration parameters for satellite imagery processing\"\"\"\n",
    "    \n",
    "    # Paths\n",
    "    PROJECT_ROOT = WORKING_DIR\n",
    "    RAW_DATA_PATH = PROJECT_ROOT / \"data/raw\"\n",
    "    OUTPUT_PATH = PROJECT_ROOT / \"data/output\"\n",
    "    \n",
    "    # Data files\n",
    "    CAPELLA_ARCHIVE_PATH = RAW_DATA_PATH / \"capella_archive.parquet\"\n",
    "    CAPELLA_STAC_PATH = RAW_DATA_PATH / \"capella_stac.parquet\"\n",
    "    #SKYSAT_PATH = RAW_DATA_PATH / \"skysat.parquet\"\n",
    "    \n",
    "    # Processing parameters\n",
    "    TEMPORAL_WINDOW_MIN = 7  # minimum days between SAR acquisitions\n",
    "    TEMPORAL_WINDOW_MAX = 14  # maximum days between SAR acquisitions\n",
    "    OVERLAP_THRESHOLD = 95.0  # % spatial overlap required\n",
    "    \n",
    "    # Angle thresholds for different quality levels\n",
    "    ANGLE_THRESHOLDS = {\n",
    "        'strict': 1,\n",
    "        'moderate': 2.0,\n",
    "        'relaxed': 5.0\n",
    "    }\n",
    "\n",
    "# Create directories\n",
    "for dir_name in [\"data/raw\", \"data/output\", \"figures\"]:\n",
    "    (WORKING_DIR / dir_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Quick file check\n",
    "missing_files = []\n",
    "for path in [config.CAPELLA_ARCHIVE_PATH, config.CAPELLA_STAC_PATH]:\n",
    "    if not path.exists():\n",
    "        missing_files.append(str(path))\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"WARNING: Missing files: {missing_files}\")\n",
    "else:\n",
    "    print(\"✓ All required files found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61550f14-7e9e-473d-8ec1-19b7bae8c6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Define helper/utility functions\n",
    "\"\"\"\n",
    "\n",
    "def create_processing_log(operation: str, start_time: datetime, \n",
    "                         records_in: int, records_out: int) -> Dict:\n",
    "    \"\"\"Create standardised log entry for processing operations\"\"\"\n",
    "    duration = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    return {\n",
    "        'operation': operation,\n",
    "        'duration_seconds': duration,\n",
    "        'records_in': records_in,\n",
    "        'records_out': records_out,\n",
    "        'processing_rate': records_in / duration if duration > 0 else 0\n",
    "    }\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15087a5-bf07-4e8e-9d92-165c5c18cbd8",
   "metadata": {},
   "source": [
    "# Objective 1: Exploring SAR data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251e5f20-cf7b-4ba2-93cb-2af33b8bb436",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## (First quick look)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a8f868f-466f-4b10-88b8-6c847023906d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>datetime</th>\n",
       "      <th>mode</th>\n",
       "      <th>products</th>\n",
       "      <th>url</th>\n",
       "      <th>look_angle</th>\n",
       "      <th>incd_angle</th>\n",
       "      <th>sqnt_angle</th>\n",
       "      <th>num_looks</th>\n",
       "      <th>observ_dir</th>\n",
       "      <th>...</th>\n",
       "      <th>orbt_plane</th>\n",
       "      <th>polariztn</th>\n",
       "      <th>slc</th>\n",
       "      <th>geo</th>\n",
       "      <th>gec</th>\n",
       "      <th>sicd</th>\n",
       "      <th>sidd</th>\n",
       "      <th>cphd</th>\n",
       "      <th>geom</th>\n",
       "      <th>geometry_converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>383ebc02-264f-4ec9-851a-352545199c0a</td>\n",
       "      <td>2025-08-24 04:51:09.100659+00:00</td>\n",
       "      <td>spotlight</td>\n",
       "      <td>SLC, GEO, GEC, SICD, SIDD</td>\n",
       "      <td>https://console.capellaspace.com/search/?q=383...</td>\n",
       "      <td>18.5</td>\n",
       "      <td>20.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>right</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>HH</td>\n",
       "      <td>CAPELLA_C11_SP_SLC_HH_20250824045104_202508240...</td>\n",
       "      <td>CAPELLA_C11_SP_GEO_HH_20250824045104_202508240...</td>\n",
       "      <td>CAPELLA_C11_SP_GEC_HH_20250824045104_202508240...</td>\n",
       "      <td>CAPELLA_C11_SP_SICD_HH_20250824045104_20250824...</td>\n",
       "      <td>CAPELLA_C11_SP_SIDD_HH_20250824045104_20250824...</td>\n",
       "      <td>None</td>\n",
       "      <td>b'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00...</td>\n",
       "      <td>0103000020E610000001000000050000002B297F102C30...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                         datetime  \\\n",
       "0  383ebc02-264f-4ec9-851a-352545199c0a 2025-08-24 04:51:09.100659+00:00   \n",
       "\n",
       "        mode                   products  \\\n",
       "0  spotlight  SLC, GEO, GEC, SICD, SIDD   \n",
       "\n",
       "                                                 url  look_angle  incd_angle  \\\n",
       "0  https://console.capellaspace.com/search/?q=383...        18.5        20.4   \n",
       "\n",
       "   sqnt_angle  num_looks observ_dir  ... orbt_plane  polariztn  \\\n",
       "0         0.0          1      right  ...         53         HH   \n",
       "\n",
       "                                                 slc  \\\n",
       "0  CAPELLA_C11_SP_SLC_HH_20250824045104_202508240...   \n",
       "\n",
       "                                                 geo  \\\n",
       "0  CAPELLA_C11_SP_GEO_HH_20250824045104_202508240...   \n",
       "\n",
       "                                                 gec  \\\n",
       "0  CAPELLA_C11_SP_GEC_HH_20250824045104_202508240...   \n",
       "\n",
       "                                                sicd  \\\n",
       "0  CAPELLA_C11_SP_SICD_HH_20250824045104_20250824...   \n",
       "\n",
       "                                                sidd  cphd  \\\n",
       "0  CAPELLA_C11_SP_SIDD_HH_20250824045104_20250824...  None   \n",
       "\n",
       "                                                geom  \\\n",
       "0  b'\\x01\\x03\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x05\\x00...   \n",
       "\n",
       "                                  geometry_converted  \n",
       "0  0103000020E610000001000000050000002B297F102C30...  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What are the data columns in the SAR parquet\n",
    "# What columns/field names should be in the analysis etc.\n",
    "import pandas as pd\n",
    "pd.read_parquet(config.CAPELLA_ARCHIVE_PATH).head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7a44a5-20b7-458e-b629-8daf079fca49",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## STEP 2: data loading, exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae9836fb-1814-4ab4-9d6d-5f2faf48a37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA EXPLORATION\n",
      "============================================================\n",
      "Capella Archive:\n",
      "  124,958 images\n",
      "  Date range: 2020-10-19 to 2025-08-25\n",
      "  2 polarizations, 3 acquisition modes\n",
      "  Look angles: 3.2° to 58.1° (avg: 33.9°)\n",
      "\n",
      "  Polarization distribution:\n",
      "    HH: 118,503 (94.8%)\n",
      "    VV: 6,455 (5.2%)\n",
      "\n",
      "  Acquisition mode distribution:\n",
      "    spotlight: 93,427 (74.8%)\n",
      "    stripmap: 21,722 (17.4%)\n",
      "    sliding_spotlight: 9,809 (7.8%)\n",
      "\n",
      "  21 columns available\n",
      "\n",
      "Capella STAC: 124,449 records\n",
      "\n",
      "------------------------------------------------------------\n",
      "Exporting exploration results...\n",
      "  Summary CSV: data_exploration_summary.csv\n",
      "  Detailed JSON: data_exploration_detailed.json\n",
      "  HTML report: data_exploration_report.html\n",
      "\n",
      "Exploration complete and exported\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 2: Data exploration using DuckDB and exporting csv\n",
    "\"\"\"\n",
    "\n",
    "def quick_explore_data(conn: duckdb.DuckDBPyConnection, config: Config) -> Dict:\n",
    "    \"\"\"Quick exploration of data structure with export to file\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"DATA EXPLORATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    results = {}\n",
    "    summary_rows = []\n",
    "    \n",
    "    # Check Capella Archive\n",
    "    if config.CAPELLA_ARCHIVE_PATH.exists():\n",
    "        # Basic statistics\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_rows,\n",
    "            COUNT(DISTINCT id) as unique_images,\n",
    "            MIN(datetime) as earliest_date,\n",
    "            MAX(datetime) as latest_date,\n",
    "            COUNT(DISTINCT polariztn) as polarizations,\n",
    "            COUNT(DISTINCT mode) as modes,\n",
    "            AVG(look_angle) as avg_look_angle,\n",
    "            MIN(look_angle) as min_look_angle,\n",
    "            MAX(look_angle) as max_look_angle\n",
    "        FROM '{config.CAPELLA_ARCHIVE_PATH}'\n",
    "        \"\"\"\n",
    "        \n",
    "        stats = conn.execute(query).fetchone()\n",
    "        \n",
    "        results['capella_archive'] = {\n",
    "            'total_rows': stats[0],\n",
    "            'unique_images': stats[1],\n",
    "            'date_range': (stats[2], stats[3]),\n",
    "            'polarizations': stats[4],\n",
    "            'modes': stats[5],\n",
    "            'look_angle_stats': (stats[6], stats[7], stats[8])\n",
    "        }\n",
    "        \n",
    "        print(f\"Capella Archive:\")\n",
    "        print(f\"  {stats[0]:,} images\")\n",
    "        print(f\"  Date range: {stats[2]:%Y-%m-%d} to {stats[3]:%Y-%m-%d}\")\n",
    "        print(f\"  {stats[4]} polarizations, {stats[5]} acquisition modes\")\n",
    "        print(f\"  Look angles: {stats[7]:.1f}° to {stats[8]:.1f}° (avg: {stats[6]:.1f}°)\")\n",
    "        \n",
    "        # Add to summary table\n",
    "        summary_rows.append({\n",
    "            'Dataset': 'Capella Archive',\n",
    "            'Total_Records': stats[0],\n",
    "            'Unique_Images': stats[1],\n",
    "            'Start_Date': stats[2].strftime('%Y-%m-%d') if stats[2] else 'N/A',\n",
    "            'End_Date': stats[3].strftime('%Y-%m-%d') if stats[3] else 'N/A',\n",
    "            'Polarizations': stats[4],\n",
    "            'Modes': stats[5],\n",
    "            'Avg_Look_Angle': f\"{stats[6]:.1f}\" if stats[6] else 'N/A'\n",
    "        })\n",
    "        \n",
    "        # Get polarization breakdown\n",
    "        pol_query = f\"\"\"\n",
    "        SELECT polariztn, COUNT(*) as count\n",
    "        FROM '{config.CAPELLA_ARCHIVE_PATH}'\n",
    "        GROUP BY polariztn\n",
    "        ORDER BY count DESC\n",
    "        \"\"\"\n",
    "        pol_results = conn.execute(pol_query).fetchall()\n",
    "        \n",
    "        print(\"\\n  Polarization distribution:\")\n",
    "        for pol, count in pol_results:\n",
    "            print(f\"    {pol}: {count:,} ({count/stats[0]*100:.1f}%)\")\n",
    "            \n",
    "        # Get mode breakdown\n",
    "        mode_query = f\"\"\"\n",
    "        SELECT mode, COUNT(*) as count\n",
    "        FROM '{config.CAPELLA_ARCHIVE_PATH}'\n",
    "        GROUP BY mode\n",
    "        ORDER BY count DESC\n",
    "        \"\"\"\n",
    "        mode_results = conn.execute(mode_query).fetchall()\n",
    "        \n",
    "        print(\"\\n  Acquisition mode distribution:\")\n",
    "        for mode, count in mode_results:\n",
    "            print(f\"    {mode}: {count:,} ({count/stats[0]*100:.1f}%)\")\n",
    "        \n",
    "        # Get temporal distribution by month\n",
    "        temporal_query = f\"\"\"\n",
    "        SELECT \n",
    "            DATE_TRUNC('month', datetime) as month,\n",
    "            COUNT(*) as count\n",
    "        FROM '{config.CAPELLA_ARCHIVE_PATH}'\n",
    "        WHERE datetime IS NOT NULL\n",
    "        GROUP BY month\n",
    "        ORDER BY month\n",
    "        \"\"\"\n",
    "        temporal_results = conn.execute(temporal_query).fetchall()\n",
    "        \n",
    "        # Store additional details\n",
    "        results['capella_archive']['polarization_dist'] = dict(pol_results)\n",
    "        results['capella_archive']['mode_dist'] = dict(mode_results)\n",
    "        results['capella_archive']['temporal_dist'] = [(str(m), c) for m, c in temporal_results]\n",
    "        \n",
    "        # Get column information\n",
    "        columns = conn.execute(f\"DESCRIBE SELECT * FROM '{config.CAPELLA_ARCHIVE_PATH}' LIMIT 0\").fetchall()\n",
    "        print(f\"\\n  {len(columns)} columns available\")\n",
    "        results['capella_archive']['columns'] = [col[0] for col in columns]\n",
    "    \n",
    "    # Check STAC metadata if exists\n",
    "    if config.CAPELLA_STAC_PATH.exists():\n",
    "        query = f\"\"\"\n",
    "        SELECT COUNT(*) as total_rows\n",
    "        FROM '{config.CAPELLA_STAC_PATH}'\n",
    "        \"\"\"\n",
    "        stac_count = conn.execute(query).fetchone()[0]\n",
    "        \n",
    "        summary_rows.append({\n",
    "            'Dataset': 'Capella STAC',\n",
    "            'Total_Records': stac_count,\n",
    "            'Unique_Images': 'N/A',\n",
    "            'Start_Date': 'N/A',\n",
    "            'End_Date': 'N/A',\n",
    "            'Polarizations': 'N/A',\n",
    "            'Modes': 'N/A',\n",
    "            'Avg_Look_Angle': 'N/A'\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nCapella STAC: {stac_count:,} records\")\n",
    "    \n",
    "    # Export exploration results to files\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"Exporting exploration results...\")\n",
    "    \n",
    "    # 1. Export summary table as CSV\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    csv_path = config.OUTPUT_PATH / 'data_exploration_summary.csv'\n",
    "    summary_df.to_csv(csv_path, index=False)\n",
    "    print(f\"  Summary CSV: {csv_path.name}\")\n",
    "    \n",
    "    # 2. Export detailed results as JSON\n",
    "    json_path = config.OUTPUT_PATH / 'data_exploration_detailed.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        # Convert datetime objects to strings for JSON serialization\n",
    "        json_results = json.loads(pd.Series(results).to_json())\n",
    "        json.dump(json_results, f, indent=2, default=str)\n",
    "    print(f\"  Detailed JSON: {json_path.name}\")\n",
    "    \n",
    "    # 3. Export as formatted HTML report\n",
    "    html_content = f\"\"\"\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>SAR Data Exploration Report</title>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "            h1 {{ color: #333; }}\n",
    "            h2 {{ color: #666; border-bottom: 1px solid #ccc; padding-bottom: 5px; }}\n",
    "            table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}\n",
    "            th {{ background-color: #f0f0f0; padding: 10px; text-align: left; border: 1px solid #ddd; }}\n",
    "            td {{ padding: 8px; border: 1px solid #ddd; }}\n",
    "            .metadata {{ background-color: #f9f9f9; padding: 10px; margin: 10px 0; border-radius: 5px; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>SAR Data Exploration Report</h1>\n",
    "        <div class=\"metadata\">\n",
    "            <p><strong>Generated:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
    "            <p><strong>Data Path:</strong> {config.RAW_DATA_PATH}</p>\n",
    "        </div>\n",
    "        \n",
    "        <h2>Dataset Summary</h2>\n",
    "        {summary_df.to_html(index=False, table_id='summary-table')}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add detailed breakdowns if available\n",
    "    if 'capella_archive' in results:\n",
    "        ca = results['capella_archive']\n",
    "        \n",
    "        # Polarization distribution\n",
    "        if 'polarization_dist' in ca:\n",
    "            pol_df = pd.DataFrame(list(ca['polarization_dist'].items()), \n",
    "                                 columns=['Polarization', 'Count'])\n",
    "            pol_df['Percentage'] = (pol_df['Count'] / pol_df['Count'].sum() * 100).round(1)\n",
    "            \n",
    "            html_content += f\"\"\"\n",
    "        <h2>Polarization Distribution</h2>\n",
    "        {pol_df.to_html(index=False)}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Mode distribution\n",
    "        if 'mode_dist' in ca:\n",
    "            mode_df = pd.DataFrame(list(ca['mode_dist'].items()), \n",
    "                                  columns=['Acquisition Mode', 'Count'])\n",
    "            mode_df['Percentage'] = (mode_df['Count'] / mode_df['Count'].sum() * 100).round(1)\n",
    "            \n",
    "            html_content += f\"\"\"\n",
    "        <h2>Acquisition Mode Distribution</h2>\n",
    "        {mode_df.to_html(index=False)}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Temporal distribution (first 12 months)\n",
    "        if 'temporal_dist' in ca and len(ca['temporal_dist']) > 0:\n",
    "            temp_df = pd.DataFrame(ca['temporal_dist'][:12], \n",
    "                                  columns=['Month', 'Image Count'])\n",
    "            \n",
    "            html_content += f\"\"\"\n",
    "        <h2>Temporal Distribution (Recent Months)</h2>\n",
    "        {temp_df.to_html(index=False)}\n",
    "        \"\"\"\n",
    "        \n",
    "        # Column list\n",
    "        if 'columns' in ca:\n",
    "            col_list = ', '.join(ca['columns'][:20])  # First 20 columns\n",
    "            if len(ca['columns']) > 20:\n",
    "                col_list += f\", ... ({len(ca['columns'])-20} more)\"\n",
    "            \n",
    "            html_content += f\"\"\"\n",
    "        <h2>Available Columns</h2>\n",
    "        <div class=\"metadata\">\n",
    "            <p><strong>Total columns:</strong> {len(ca['columns'])}</p>\n",
    "            <p><strong>Column names:</strong> {col_list}</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    html_path = config.OUTPUT_PATH / 'data_exploration_report.html'\n",
    "    with open(html_path, 'w') as f:\n",
    "        f.write(html_content)\n",
    "    print(f\"  HTML report: {html_path.name}\")\n",
    "    \n",
    "    print(\"\\nExploration complete and exported\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run exploration with export\n",
    "exploration_results = quick_explore_data(conn, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1bf18b-9c4d-4db5-bc45-a6f4e6d0fc32",
   "metadata": {},
   "source": [
    "# Objective 2: Assemble Capella pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8fc484-647a-43d0-99d9-337bdc72a6a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### [Intermediate step to debug the 'geom' format]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "468f4873-07e4-45e0-81c2-ec5f9c6d1ad4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Debug geometry format to understand how to parse the \"geom\" column\n",
    "# Can we use DuckDB?\n",
    "# \"\"\"\n",
    "# # Check if DuckDB spatial functions work on this data\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "# print(\"TESTING DUCKDB SPATIAL APPROACH\")\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# def test_duckdb_spatial_operations():\n",
    "#     \"\"\"\n",
    "#     Test if we can do spatial operations directly in DuckDB\n",
    "#     This might be more efficient than converting to shapely\n",
    "#     \"\"\"\n",
    "    \n",
    "#     file_path = config.CAPELLA_ARCHIVE_PATH\n",
    "    \n",
    "#     try:\n",
    "#         # Test basic spatial functions\n",
    "#         query = f\"\"\"\n",
    "#         SELECT \n",
    "#             COUNT(*) as total_geoms,\n",
    "#             COUNT(CASE WHEN ST_IsValid(geom) THEN 1 END) as valid_geoms,\n",
    "#             ST_GeometryType(geom) as geom_type,\n",
    "#             COUNT(*)\n",
    "#         FROM '{file_path}'\n",
    "#         WHERE geom IS NOT NULL\n",
    "#         GROUP BY ST_GeometryType(geom)\n",
    "#         LIMIT 5\n",
    "#         \"\"\"\n",
    "        \n",
    "#         spatial_stats = conn.execute(query).fetchall()\n",
    "        \n",
    "#         print(\"Spatial statistics:\")\n",
    "#         for row in spatial_stats:\n",
    "#             print(f\"  {row[2]}: {row[3]:,} geometries, {row[1]:,} valid\")\n",
    "        \n",
    "#         # Test spatial bounds calculation\n",
    "#         query_bounds = f\"\"\"\n",
    "#         SELECT \n",
    "#             MIN(ST_XMin(geom)) as min_x,\n",
    "#             MAX(ST_XMax(geom)) as max_x,\n",
    "#             MIN(ST_YMin(geom)) as min_y,\n",
    "#             MAX(ST_YMax(geom)) as max_y\n",
    "#         FROM '{file_path}'\n",
    "#         WHERE geom IS NOT NULL\n",
    "#         LIMIT 1000\n",
    "#         \"\"\"\n",
    "        \n",
    "#         bounds_result = conn.execute(query_bounds).fetchone()\n",
    "        \n",
    "#         if bounds_result:\n",
    "#             print(f\"\\nSpatial extent (sample):\")\n",
    "#             print(f\"  Longitude: {bounds_result[0]:.6f} to {bounds_result[1]:.6f}\")\n",
    "#             print(f\"  Latitude: {bounds_result[2]:.6f} to {bounds_result[3]:.6f}\")\n",
    "        \n",
    "#         # Test overlap calculation between two geometries\n",
    "#         query_overlap_test = f\"\"\"\n",
    "#         WITH test_pairs AS (\n",
    "#             SELECT \n",
    "#                 a.id as id1,\n",
    "#                 b.id as id2,\n",
    "#                 a.geom as geom1,\n",
    "#                 b.geom as geom2,\n",
    "#                 a.datetime as date1,\n",
    "#                 b.datetime as date2\n",
    "#             FROM '{file_path}' a\n",
    "#             JOIN '{file_path}' b ON (\n",
    "#                 b.datetime > a.datetime \n",
    "#                 AND (b.datetime::DATE - a.datetime::DATE) BETWEEN 7 AND 14\n",
    "#             )\n",
    "#             WHERE a.geom IS NOT NULL AND b.geom IS NOT NULL\n",
    "#             LIMIT 5\n",
    "#         )\n",
    "#         SELECT \n",
    "#             id1,\n",
    "#             id2,\n",
    "#             ST_Area(ST_Intersection(geom1, geom2)) as intersection_area,\n",
    "#             ST_Area(geom1) as area1,\n",
    "#             ST_Area(geom2) as area2,\n",
    "#             (ST_Area(ST_Intersection(geom1, geom2)) / LEAST(ST_Area(geom1), ST_Area(geom2))) * 100 as overlap_pct\n",
    "#         FROM test_pairs\n",
    "#         WHERE ST_Intersects(geom1, geom2)\n",
    "#         \"\"\"\n",
    "        \n",
    "#         overlap_results = conn.execute(query_overlap_test).fetchall()\n",
    "        \n",
    "#         print(f\"\\nTesting spatial overlaps on {len(overlap_results)} intersecting pairs:\")\n",
    "#         for row in overlap_results:\n",
    "#             print(f\"  Pair {row[0]}-{row[1]}: {row[5]:.1f}% overlap\")\n",
    "        \n",
    "#         return len(overlap_results) > 0\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"DuckDB spatial operations failed: {e}\")\n",
    "#         return False\n",
    "\n",
    "# # Test DuckDB spatial approach\n",
    "# duckdb_spatial_works = test_duckdb_spatial_operations()\n",
    "\n",
    "# if duckdb_spatial_works:\n",
    "#     print(f\"\\nSOLUTION FOUND: Use DuckDB spatial functions directly!\")\n",
    "#     print(\"The geometry format works with DuckDB's built-in spatial operations.\")\n",
    "#     print(\"We should revise the pair matching to use SQL spatial functions instead of shapely.\")\n",
    "# else:\n",
    "#     print(f\"\\nNeed to investigate further - DuckDB spatial functions also failed.\")\n",
    "    \n",
    "#     # Additional debugging - check if it's a known format\n",
    "#     print(\"\\nTrying additional geometry format detection...\")\n",
    "    \n",
    "#     # Check first few bytes for format signatures\n",
    "#     sample_bytes = samples[0][2][:20] if samples else None\n",
    "#     if sample_bytes:\n",
    "#         hex_start = sample_bytes[:8].hex()\n",
    "#         print(f\"First 8 bytes (hex): {hex_start}\")\n",
    "        \n",
    "#         # Common geometry format signatures\n",
    "#         format_signatures = {\n",
    "#             '01': 'Little-endian WKB',\n",
    "#             '00': 'Big-endian WKB', \n",
    "#             '0001': 'Point (little-endian)',\n",
    "#             '0002': 'LineString (little-endian)',\n",
    "#             '0003': 'Polygon (little-endian)',\n",
    "#             '2000': 'SRID prefix (big-endian)',\n",
    "#             '0020': 'SRID prefix (little-endian)'\n",
    "#         }\n",
    "        \n",
    "#         for sig, desc in format_signatures.items():\n",
    "#             if hex_start.startswith(sig.lower()):\n",
    "#                 print(f\"Possible format: {desc}\")\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc20980-733d-40d0-b3b5-66b743d95c8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## STEP 3: Capella pairs matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e083c59f-1231-4716-aff9-77a1adadd9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 3: SAR pair matching using DuckDB + spatial operations\n",
    "Runs everything in DuckDB, no intermediate files, no pandas registration.\n",
    "\"\"\"\n",
    "\n",
    "class SARPairMatcher:\n",
    "    \"\"\"SAR pair matching with better performance using DuckDB\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config, conn: duckdb.DuckDBPyConnection):\n",
    "        self.config = config\n",
    "        self.conn = conn\n",
    "\n",
    "    def find_all_pairs(self, angle_threshold: float = 5.0,\n",
    "                      sample_limit: int = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Find all valid SAR pairs in one stepped query\n",
    "        Uses DuckDB's spatial functions for overlap calculation\n",
    "        Preserves geometry columns for validation purposes.\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"\\nFinding pairs with ≤{angle_threshold}° angle difference...\")\n",
    "\n",
    "        file_path = self.config.CAPELLA_ARCHIVE_PATH\n",
    "        limit_clause = f\"LIMIT {sample_limit}\" if sample_limit else \"\"\n",
    "\n",
    "        query = f\"\"\"\n",
    "        WITH sar_data AS (\n",
    "            SELECT \n",
    "                id, datetime, geom, polariztn, mode, look_angle,\n",
    "                datetime::DATE as date_only\n",
    "            FROM '{file_path}'\n",
    "            WHERE geom IS NOT NULL \n",
    "                AND datetime IS NOT NULL \n",
    "                AND polariztn IS NOT NULL \n",
    "                AND mode IS NOT NULL \n",
    "                AND look_angle IS NOT NULL\n",
    "            {limit_clause}\n",
    "        ),\n",
    "        candidate_pairs AS (\n",
    "            SELECT \n",
    "                a.id as id1,\n",
    "                b.id as id2,\n",
    "                a.datetime as date1,\n",
    "                b.datetime as date2,\n",
    "                EXTRACT(EPOCH FROM (b.datetime - a.datetime))/86400 as temporal_baseline_days,\n",
    "                a.polariztn as polarisation,\n",
    "                a.mode as mode,\n",
    "                a.look_angle as angle1,\n",
    "                b.look_angle as angle2,\n",
    "                ABS(a.look_angle - b.look_angle) as angle_diff,\n",
    "                a.geom as geom1,\n",
    "                b.geom as geom2\n",
    "            FROM sar_data a\n",
    "            INNER JOIN sar_data b ON (\n",
    "                b.date_only > a.date_only\n",
    "                AND b.date_only <= a.date_only + INTERVAL '14 days'\n",
    "                AND b.date_only >= a.date_only + INTERVAL '7 days'\n",
    "                AND a.polariztn = b.polariztn\n",
    "                AND a.mode = b.mode\n",
    "                AND ABS(a.look_angle - b.look_angle) <= {angle_threshold}\n",
    "            )\n",
    "        ),\n",
    "        spatial_pairs AS (\n",
    "            SELECT \n",
    "                *,\n",
    "                ST_Area(ST_Intersection(geom1, geom2)) / \n",
    "                    LEAST(ST_Area(geom1), ST_Area(geom2)) * 100 as overlap_pct\n",
    "            FROM candidate_pairs\n",
    "            WHERE ST_Intersects(geom1, geom2)\n",
    "        )\n",
    "        SELECT \n",
    "            id1, id2, date1, date2, \n",
    "            temporal_baseline_days,\n",
    "            polarisation, mode,\n",
    "            angle1, angle2, angle_diff,\n",
    "            overlap_pct,\n",
    "            geom1, geom2,\n",
    "            ST_AsText(geom1) as wkt_geom1,\n",
    "            ST_AsText(geom2) as wkt_geom2\n",
    "        FROM spatial_pairs\n",
    "        WHERE overlap_pct >= {self.config.OVERLAP_THRESHOLD}\n",
    "        ORDER BY overlap_pct DESC, angle_diff ASC\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            result_df = self.conn.execute(query).fetch_df()\n",
    "            print(f\"  Found {len(result_df)} valid pairs\")\n",
    "\n",
    "            if len(result_df) > 0:\n",
    "                print(f\"  Mean overlap: {result_df['overlap_pct'].mean():.1f}%\")\n",
    "                print(f\"  Mean angle diff: {result_df['angle_diff'].mean():.2f}°\")\n",
    "\n",
    "            return result_df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def process_multiple_thresholds(self, thresholds=None) -> dict:\n",
    "        \"\"\"\n",
    "        Process pairs for selected angle thresholds.\n",
    "        Pass a list of keys (e.g., ['strict']) to process only those thresholds.\n",
    "        If None, process all defined in config.ANGLE_THRESHOLDS.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"=\"*70)\n",
    "        print(\"SAR PAIR MATCHING\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        # Determine which thresholds to run\n",
    "        threshold_dict = self.config.ANGLE_THRESHOLDS\n",
    "        if thresholds is None:\n",
    "            keys_to_run = list(threshold_dict.keys())\n",
    "        else:\n",
    "            keys_to_run = [k for k in thresholds if k in threshold_dict]\n",
    "\n",
    "        for name in keys_to_run:\n",
    "            threshold = threshold_dict[name]\n",
    "            print(f\"\\n{name.upper()} threshold ({threshold}°):\")\n",
    "            pairs_df = self.find_all_pairs(angle_threshold=threshold)\n",
    "\n",
    "            if not pairs_df.empty:\n",
    "                output_path = self.config.OUTPUT_PATH / f'pairs_{name}_{threshold}deg.parquet'\n",
    "                pairs_df.to_parquet(output_path, index=False)\n",
    "                results[name] = pairs_df\n",
    "\n",
    "                print(f\"  Saved to: {output_path.name}\")\n",
    "\n",
    "        # Summary\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        for name, df in results.items():\n",
    "            if not df.empty:\n",
    "                print(f\"{name}: {len(df)} pairs (overlap: {df['overlap_pct'].mean():.1f}±{df['overlap_pct'].std():.1f}%)\")\n",
    "\n",
    "        return results\n",
    "\n",
    "# Example usage:\n",
    "matcher = SARPairMatcher(config, conn)\n",
    "strict_results = matcher.process_multiple_thresholds(thresholds=['strict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdf5c03-c193-43f1-99de-276fd0ad9cc0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Step 4: Validation (remove dups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "267543a4-3576-4d6d-abe6-90a5e772b45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id1', 'id2', 'date1', 'date2', 'temporal_baseline_days',\n",
      "       'polarisation', 'mode', 'angle1', 'angle2', 'angle_diff', 'overlap_pct',\n",
      "       'geom1', 'geom2', 'wkt_geom1', 'wkt_geom2'],\n",
      "      dtype='object')\n",
      "Validated file saved to: pairs_strict_1deg_validated.parquet\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 4: validation, dups removal\n",
    "\"\"\"\n",
    "# Last checks using panda and WKT cols for validation \n",
    "import pandas as pd\n",
    "\n",
    "strict_file_path = config.OUTPUT_PATH / \"pairs_strict_1deg.parquet\"\n",
    "df = pd.read_parquet(strict_file_path)\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "# Drop duplicates based on date1, date2, and identical geometry (WKT string)\n",
    "dups = df.duplicated(subset=['date1', 'date2', 'wkt_geom1'], keep='first')\n",
    "validated_df = df[~dups]\n",
    "\n",
    "validated_path = strict_file_path.with_name(strict_file_path.stem + \"_validated.parquet\")\n",
    "validated_df.to_parquet(validated_path, index=False)\n",
    "print(f\"Validated file saved to: {validated_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25086bff-0ef4-44ab-822d-ed46d061aecd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## STEP 5: Mapping the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a28f73b4-c0a6-451f-9ad2-ca09a0ed2673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STRICT SAR PAIR VISUALISATION\n",
      "============================================================\n",
      "Loading strictest pairs: pairs_strict_1deg.parquet\n",
      "Loaded 13385 STRICT pairs from pairs_strict_1deg.parquet\n",
      "  Mean overlap: 99.0%\n",
      "  Mean angle diff: 0.34°\n",
      "Retrieving geometries for top 20 strict pairs...\n",
      "Retrieved geometries for 37 images\n",
      "Creating  interactive map: Strict SAR Training Pairs (Top 20 of 13385 total)\n",
      "Saved strict (1 deg) pairs map: C:\\Users\\Emma Underwood\\Documents\\GitHub\\LiveEO\\data\\output\\strict_pairs_map_ELU_16092025.html\n",
      "\n",
      "Strict pairs (1deg) map features:\n",
      "  • 20 of 13385 strict pairs visualised\n",
      "  • Country borders overlay for geographic context\n",
      "  • Quick zoom sidebar for navigation\n",
      "  • Individual zoom buttons in popups\n",
      "  • Multiple basemap options\n",
      "  • All pairs meet ≤1° angle difference requirement\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 5: Interactive map for \"strict\", validated chosen pairs (1 deg angle diff) \n",
    "with country borders and zoom controls\n",
    "\"\"\"\n",
    "\n",
    "import folium\n",
    "from folium import plugins\n",
    "import json\n",
    "import pandas as pd\n",
    "from shapely import wkt\n",
    "from typing import Dict\n",
    "\n",
    "class SARPairVisualiser:\n",
    "    \"\"\"\n",
    "    Create interactive maps with country borders and zoom controls\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.conn = duckdb.connect()\n",
    "        self.conn.execute(\"INSTALL spatial; LOAD spatial;\")\n",
    "    \n",
    "    def load_sar_pairs(self, pairs_file: str = None) -> pd.DataFrame:\n",
    "        \"\"\"Load SAR pairs - specifically the strictest threshold\"\"\"\n",
    "        if pairs_file is None:\n",
    "            # Default to strictest pairs file\n",
    "            pairs_file = 'pairs_strict_1deg.parquet'  # Or change to your exact filename\n",
    "            print(f\"Loading strictest pairs: {pairs_file}\")\n",
    "        \n",
    "        pairs_path = self.config.OUTPUT_PATH / pairs_file\n",
    "        \n",
    "        # If strict file doesn't exist, check for alternative names\n",
    "        if not pairs_path.exists():\n",
    "            alternative_names = [\n",
    "                'pairs_strict_1deg.parquet',\n",
    "                'training_pairs_strict_1deg.parquet'\n",
    "            ]\n",
    "            \n",
    "            for alt_name in alternative_names:\n",
    "                alt_path = self.config.OUTPUT_PATH / alt_name\n",
    "                if alt_path.exists():\n",
    "                    pairs_path = alt_path\n",
    "                    pairs_file = alt_name\n",
    "                    print(f\"Found strict pairs file: {pairs_file}\")\n",
    "                    break\n",
    "            \n",
    "            # If still not found, error out\n",
    "            if not pairs_path.exists():\n",
    "                print(f\"ERROR: Strict pairs file not found. Looking for: {pairs_file}\")\n",
    "                print(f\"Available files in output directory:\")\n",
    "                for f in self.config.OUTPUT_PATH.glob('*pairs*.parquet'):\n",
    "                    print(f\"  - {f.name}\")\n",
    "                return pd.DataFrame()\n",
    "        \n",
    "        # Load the strict pairs\n",
    "        try:\n",
    "            pairs_df = pd.read_parquet(pairs_path)\n",
    "            print(f\"Loaded {len(pairs_df)} STRICT pairs from {pairs_file}\")\n",
    "            \n",
    "            # Show quality metrics\n",
    "            if 'overlap_pct' in pairs_df.columns:\n",
    "                print(f\"  Mean overlap: {pairs_df['overlap_pct'].mean():.1f}%\")\n",
    "            if 'angle_diff' in pairs_df.columns:\n",
    "                print(f\"  Mean angle diff: {pairs_df['angle_diff'].mean():.2f}°\")\n",
    "            \n",
    "            return pairs_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading pairs file: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def get_pair_geometries(self, pairs_df: pd.DataFrame, max_pairs: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"Retrieve actual geometries for selected pairs\"\"\"\n",
    "        if pairs_df.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        selected_pairs = pairs_df.head(max_pairs)\n",
    "        print(f\"Retrieving geometries for top {len(selected_pairs)} strict pairs...\")\n",
    "        \n",
    "        id1_list = selected_pairs['id1'].tolist()\n",
    "        id2_list = selected_pairs['id2'].tolist()\n",
    "        all_ids = list(set(id1_list + id2_list))\n",
    "        \n",
    "        id_str = \"', '\".join(all_ids)\n",
    "        \n",
    "        file_path = self.config.CAPELLA_ARCHIVE_PATH\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            id,\n",
    "            datetime,\n",
    "            ST_AsText(geom) as wkt_geom,\n",
    "            polariztn,\n",
    "            mode,\n",
    "            look_angle\n",
    "        FROM '{file_path}'\n",
    "        WHERE id IN ('{id_str}')\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            results = self.conn.execute(query).fetchall()\n",
    "            geom_df = pd.DataFrame(results, columns=['id', 'datetime', 'wkt_geom', \n",
    "                                                   'polarisation', 'mode', 'look_angle'])\n",
    "            \n",
    "            print(f\"Retrieved geometries for {len(geom_df)} images\")\n",
    "            return geom_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving geometries: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def create_enhanced_map(self, pairs_df: pd.DataFrame, geom_df: pd.DataFrame, \n",
    "                          map_title: str = None) -> folium.Map:\n",
    "        \"\"\"\n",
    "        Create interactive map with country borders and zoom controls\n",
    "        \"\"\"\n",
    "        if map_title is None:\n",
    "            map_title = f\"Strict SAR Training Pairs (≤2° angle difference)\"\n",
    "            \n",
    "        print(f\"Creating  interactive map: {map_title}\")\n",
    "        \n",
    "        if pairs_df.empty or geom_df.empty:\n",
    "            print(\"No data to map\")\n",
    "            return None\n",
    "        \n",
    "        # Parse all geometries and calculate bounds\n",
    "        all_bounds = []\n",
    "        geom_dict = {}\n",
    "        \n",
    "        for _, row in geom_df.iterrows():\n",
    "            try:\n",
    "                geom = wkt.loads(row['wkt_geom'])\n",
    "                geom_dict[row['id']] = geom\n",
    "                bounds = geom.bounds\n",
    "                all_bounds.extend([[bounds[1], bounds[0]], [bounds[3], bounds[2]]])\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if not all_bounds:\n",
    "            print(\"No valid geometries for mapping\")\n",
    "            return None\n",
    "        \n",
    "        # Calculate center\n",
    "        lats = [b[0] for b in all_bounds]\n",
    "        lons = [b[1] for b in all_bounds]\n",
    "        center_lat = sum(lats) / len(lats)\n",
    "        center_lon = sum(lons) / len(lons)\n",
    "        \n",
    "        # Create base map\n",
    "        m = folium.Map(\n",
    "            location=[center_lat, center_lon],\n",
    "            zoom_start=6,\n",
    "            tiles=None,\n",
    "            max_zoom=18,\n",
    "            min_zoom=2\n",
    "        )\n",
    "        \n",
    "        # Add base layers with country borders\n",
    "        folium.TileLayer(\n",
    "            tiles='OpenStreetMap',\n",
    "            name='OpenStreetMap',\n",
    "            overlay=False,\n",
    "            control=True\n",
    "        ).add_to(m)\n",
    "        \n",
    "        folium.TileLayer(\n",
    "            tiles='CartoDB positron',\n",
    "            name='Light Map',\n",
    "            overlay=False,\n",
    "            control=True\n",
    "        ).add_to(m)\n",
    "        \n",
    "        folium.TileLayer(\n",
    "            tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "            attr='Esri',\n",
    "            name='Satellite',\n",
    "            overlay=False,\n",
    "            control=True\n",
    "        ).add_to(m)\n",
    "        \n",
    "        # Add country borders overlay\n",
    "        folium.TileLayer(\n",
    "            tiles='https://stamen-tiles.a.ssl.fastly.net/toner-lines/{z}/{x}/{y}.png',\n",
    "            attr='Stamen',\n",
    "            name='Country Borders',\n",
    "            overlay=True,\n",
    "            control=True,\n",
    "            opacity=0.5\n",
    "        ).add_to(m)\n",
    "        \n",
    "        # Create feature groups for pairs\n",
    "        pair_groups = folium.FeatureGroup(name='Strict SAR Pairs')\n",
    "        \n",
    "        # Define colors - use distinct colors for better visibility\n",
    "        colors = ['red', 'blue', 'green', 'purple', 'orange', 'darkred', \n",
    "                 'cadetblue', 'darkgreen', 'darkblue', 'pink']\n",
    "        \n",
    "        # Store pair bounds for zoom functionality\n",
    "        pair_bounds = []\n",
    "        \n",
    "        # Add pairs to map\n",
    "        for idx, row in pairs_df.iterrows():\n",
    "            pair_num = idx + 1\n",
    "            color = colors[idx % len(colors)]\n",
    "            \n",
    "            if row['id1'] in geom_dict and row['id2'] in geom_dict:\n",
    "                try:\n",
    "                    geom1 = geom_dict[row['id1']]\n",
    "                    geom2 = geom_dict[row['id2']]\n",
    "                    \n",
    "                    # Calculate pair bounds for zoom\n",
    "                    combined_bounds = [\n",
    "                        [min(geom1.bounds[1], geom2.bounds[1]), \n",
    "                         min(geom1.bounds[0], geom2.bounds[0])],\n",
    "                        [max(geom1.bounds[3], geom2.bounds[3]), \n",
    "                         max(geom1.bounds[2], geom2.bounds[2])]\n",
    "                    ]\n",
    "                    pair_bounds.append((pair_num, combined_bounds))\n",
    "                    \n",
    "                    # Format popup info\n",
    "                    temporal_baseline = row.get('temporal_baseline_days', 'N/A')\n",
    "                    overlap = row.get('overlap_pct', 'N/A')\n",
    "                    angle_diff = row.get('angle_diff', 'N/A')\n",
    "                    \n",
    "                    overlap_str = f\"{overlap:.1f}%\" if isinstance(overlap, (int, float)) else str(overlap)\n",
    "                    angle_str = f\"{angle_diff:.2f}°\" if isinstance(angle_diff, (int, float)) else str(angle_diff)\n",
    "                    \n",
    "                    # popup with zoom button\n",
    "                    popup_html = f\"\"\"\n",
    "                    <div style='width: 250px'>\n",
    "                        <h4>Strict Pair {pair_num}</h4>\n",
    "                        <table style='width:100%'>\n",
    "                            <tr><td><b>Temporal:</b></td><td>{temporal_baseline} days</td></tr>\n",
    "                            <tr><td><b>Overlap:</b></td><td>{overlap_str}</td></tr>\n",
    "                            <tr><td><b>Angle Diff:</b></td><td>{angle_str}</td></tr>\n",
    "                            <tr><td><b>Polarisation:</b></td><td>{row.get('polarisation', 'N/A')}</td></tr>\n",
    "                            <tr><td><b>Mode:</b></td><td>{row.get('mode', 'N/A')}</td></tr>\n",
    "                        </table>\n",
    "                        <button onclick='\n",
    "                            var bounds = {combined_bounds};\n",
    "                            window[Object.keys(window).find(key => key.startsWith(\"map_\"))].fitBounds(bounds);\n",
    "                        ' style='margin-top:10px; width:100%; padding:5px; background:#4CAF50; color:white; \n",
    "                                 border:none; border-radius:4px; cursor:pointer'>\n",
    "                            Zoom to This Pair\n",
    "                        </button>\n",
    "                    </div>\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    # Add footprints\n",
    "                    folium.GeoJson(\n",
    "                        geom1.__geo_interface__,\n",
    "                        style_function=lambda x, c=color: {\n",
    "                            'fillColor': c, 'color': c, 'weight': 2, 'fillOpacity': 0.3\n",
    "                        },\n",
    "                        popup=folium.Popup(popup_html, max_width=300),\n",
    "                        tooltip=f\"Strict Pair {pair_num} - Image 1\"\n",
    "                    ).add_to(pair_groups)\n",
    "                    \n",
    "                    folium.GeoJson(\n",
    "                        geom2.__geo_interface__,\n",
    "                        style_function=lambda x, c=color: {\n",
    "                            'fillColor': c, 'color': c, 'weight': 2, \n",
    "                            'fillOpacity': 0.1, 'dashArray': '5,5'\n",
    "                        },\n",
    "                        tooltip=f\"Strict Pair {pair_num} - Image 2\"\n",
    "                    ).add_to(pair_groups)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error adding pair {pair_num}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        pair_groups.add_to(m)\n",
    "        \n",
    "        # Add zoom controls for all pairs (sidebar)\n",
    "        zoom_control_html = \"\"\"\n",
    "        <div style='position: fixed; top: 70px; right: 10px; width: 150px; \n",
    "                    background: white; padding: 10px; border-radius: 5px; \n",
    "                    box-shadow: 0 0 15px rgba(0,0,0,0.2); z-index: 1000;\n",
    "                    max-height: 400px; overflow-y: auto;'>\n",
    "            <h4 style='margin: 0 0 10px 0; font-size: 14px;'>Quick Zoom</h4>\n",
    "        \"\"\"\n",
    "        \n",
    "        for pair_num, bounds in pair_bounds[:10]:  # Show first 10 pairs\n",
    "            zoom_control_html += f\"\"\"\n",
    "            <button onclick='\n",
    "                var bounds = {bounds};\n",
    "                window[Object.keys(window).find(key => key.startsWith(\"map_\"))].fitBounds(bounds);\n",
    "            ' style='width:100%; margin:2px 0; padding:5px; background:#f0f0f0; \n",
    "                     border:1px solid #ddd; border-radius:3px; cursor:pointer;\n",
    "                     font-size:12px;'\n",
    "                     onmouseover='this.style.background=\"#e0e0e0\"' \n",
    "                     onmouseout='this.style.background=\"#f0f0f0\"'>\n",
    "                Pair {pair_num}\n",
    "            </button>\n",
    "            \"\"\"\n",
    "        \n",
    "        zoom_control_html += \"</div>\"\n",
    "        \n",
    "        # Add the zoom control panel\n",
    "        m.get_root().html.add_child(folium.Element(zoom_control_html))\n",
    "        \n",
    "        # Add other controls\n",
    "        folium.LayerControl(position='topright').add_to(m)\n",
    "        \n",
    "        # Add fullscreen button\n",
    "        plugins.Fullscreen(\n",
    "            position='topleft',\n",
    "            title='Fullscreen',\n",
    "            title_cancel='Exit Fullscreen',\n",
    "            force_separate_button=True\n",
    "        ).add_to(m)\n",
    "        \n",
    "        # Add measurement tool\n",
    "        plugins.MeasureControl(position='topleft').add_to(m)\n",
    "        \n",
    "        # Add minimap\n",
    "        minimap = plugins.MiniMap(toggle_display=True, width=200, height=150)\n",
    "        m.add_child(minimap)\n",
    "        \n",
    "        # Add title with strict indicator\n",
    "        title_html = f'''\n",
    "        <div style='position: fixed; top: 10px; left: 50%; transform: translateX(-50%); \n",
    "                    z-index: 1000; background: white; padding: 10px; border-radius: 5px;\n",
    "                    box-shadow: 0 0 15px rgba(0,0,0,0.2);'>\n",
    "            <h3 style='margin: 0; font-size: 16px;'>{map_title}</h3>\n",
    "            <p style='margin: 5px 0 0 0; font-size: 12px; text-align: center;'>\n",
    "                {len(pairs_df)} high-quality pairs | Solid = Image 1, Dashed = Image 2\n",
    "            </p>\n",
    "        </div>\n",
    "        '''\n",
    "        m.get_root().html.add_child(folium.Element(title_html))\n",
    "        \n",
    "        return m\n",
    "    \n",
    "    def create_visualisations(self, max_pairs: int = 25) -> Dict:\n",
    "        \"\"\"Complete workflow to create visualisations for strict (<1 deg diff) pairs only\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"STRICT SAR PAIR VISUALISATION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load strict pairs only\n",
    "        pairs_df = self.load_sar_pairs()  # Will load strict pairs by default\n",
    "        if pairs_df.empty:\n",
    "            return {'error': 'No strict pairs data found'}\n",
    "        \n",
    "        # Get geometries\n",
    "        geom_df = self.get_pair_geometries(pairs_df, max_pairs=max_pairs)\n",
    "        if geom_df.empty:\n",
    "            return {'error': 'No geometries retrieved'}\n",
    "        \n",
    "        # Create map with strict pairs\n",
    "        interactive_map = self.create_enhanced_map(\n",
    "            pairs_df.head(max_pairs), \n",
    "            geom_df,\n",
    "            f\"Strict SAR Training Pairs (Top {min(max_pairs, len(pairs_df))} of {len(pairs_df)} total)\"\n",
    "        )\n",
    "        \n",
    "        if interactive_map:\n",
    "            # Use specific filename for strict pairs\n",
    "            map_path = self.config.OUTPUT_PATH / 'strict_pairs_map_ELU_16092025.html'\n",
    "            interactive_map.save(str(map_path))\n",
    "            print(f\"Saved strict (1 deg) pairs map: {map_path}\")\n",
    "            \n",
    "            return {\n",
    "                'map': interactive_map,\n",
    "                'map_file': str(map_path),\n",
    "                'pairs_mapped': min(max_pairs, len(pairs_df)),\n",
    "                'total_strict_pairs': len(pairs_df)\n",
    "            }\n",
    "        else:\n",
    "            return {'error': 'Failed to create map'}\n",
    "\n",
    "# Create visualisations for STRICT pairs\n",
    "visualiser = SARPairVisualiser(config)\n",
    "viz_results = visualiser.create_visualisations(max_pairs=20)\n",
    "\n",
    "if 'error' not in viz_results:\n",
    "    print(f\"\\nStrict pairs (1deg) map features:\")\n",
    "    print(f\"  • {viz_results['pairs_mapped']} of {viz_results.get('total_strict_pairs', 'N/A')} strict pairs visualised\")\n",
    "    print(f\"  • Country borders overlay for geographic context\")\n",
    "    print(f\"  • Quick zoom sidebar for navigation\")\n",
    "    print(f\"  • Individual zoom buttons in popups\")\n",
    "    print(f\"  • Multiple basemap options\")\n",
    "    print(f\"  • All pairs meet ≤1° angle difference requirement\")\n",
    "    viz_results['map']\n",
    "else:\n",
    "    print(f\"Visualisation failed: {viz_results['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e19d738-bc29-44b8-b03a-ed7abe7af69c",
   "metadata": {},
   "source": [
    "## End of workflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
